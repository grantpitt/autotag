{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting packaging\n",
      "  Using cached packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Collecting urllib3\n",
      "  Using cached urllib3-1.26.10-py2.py3-none-any.whl (139 kB)\n",
      "Collecting pip\n",
      "  Downloading pip-22.2-py3-none-any.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Pillow\n",
      "  Downloading Pillow-9.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 36.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting transformers\n",
      "  Using cached transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
      "Requirement already up-to-date: requests in /home/ubuntu/.local/lib/python3.8/site-packages (2.28.1)\n",
      "Collecting datasets\n",
      "  Using cached datasets-2.3.2-py3-none-any.whl (362 kB)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging) (2.4.6)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /usr/lib/python3/dist-packages (from transformers) (3.0.12)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2022.7.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (765 kB)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.3.1)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Using cached huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (1.23.1)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (4.64.0)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Using cached tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "Requirement already satisfied, skipping upgrade: charset-normalizer<3,>=2 in /home/ubuntu/.local/lib/python3.8/site-packages (from requests) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests) (2.8)\n",
      "Collecting xxhash\n",
      "  Using cached xxhash-3.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "Collecting fsspec[http]>=2021.05.0\n",
      "  Using cached fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
      "Requirement already satisfied, skipping upgrade: pandas in /usr/lib/python3/dist-packages (from datasets) (0.25.3)\n",
      "Collecting multiprocess\n",
      "  Using cached multiprocess-0.70.13-py38-none-any.whl (131 kB)\n",
      "Collecting dill<0.3.6\n",
      "  Using cached dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "Collecting responses<0.19\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Using cached pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
      "Collecting aiohttp\n",
      "  Using cached aiohttp-3.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.3.0)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets) (19.3.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.7.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (308 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Installing collected packages: packaging, urllib3, pip, Pillow, regex, huggingface-hub, tokenizers, transformers, xxhash, multidict, frozenlist, aiosignal, yarl, async-timeout, aiohttp, fsspec, dill, multiprocess, responses, pyarrow, datasets\n",
      "Successfully installed Pillow-9.2.0 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 datasets-2.3.2 dill-0.3.5.1 frozenlist-1.3.0 fsspec-2022.5.0 huggingface-hub-0.8.1 multidict-6.0.2 multiprocess-0.70.13 packaging-21.3 pip-22.2 pyarrow-8.0.0 regex-2022.7.9 responses-0.18.0 tokenizers-0.12.1 transformers-4.20.1 urllib3-1.26.10 xxhash-3.0.0 yarl-1.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade packaging urllib3 pip Pillow transformers requests datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dce7e2ab8634a7b978f509461fb3444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/568 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e666abfa6bd14d50b19637e55cdb78c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/842k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b9440d701d246219de14a5ee3ddd5c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/512k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5709979d3aba4b33a439f968c091a29b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.12M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "905d1892b1cc477b9ff1713e955b8618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9949, 0.0051]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration 2017-data_dir=.%2Fdummy_data%2F\n",
      "Reusing dataset coco_dataset_script (/home/ubuntu/.cache/huggingface/datasets/ydshieh___coco_dataset_script/2017-data_dir=.%2Fdummy_data%2F/0.0.0/e033205c0266a54c10be132f9264f2a39dcf893e798f6756d224b1ff5078998f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7232bd97bf854c1b8cfc27e7ffbadd33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "ds = datasets.load_dataset(\"ydshieh/coco_dataset_script\", \"2017\", data_dir=\"./dummy_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "\u001b[33m  WARNING: The script tqdm is installed in '/Users/vin/Library/Python/3.8/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed tqdm-4.64.0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 22.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -mpip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Import Jupyter tqdm\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User GUID</th>\n",
       "      <th>User Tags</th>\n",
       "      <th>Signage Tags</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5414FF1B-23AE-4AEF-A005-00122A80B8FC</td>\n",
       "      <td>Marketing;Consulting;Agency</td>\n",
       "      <td>Business Name;Business Logo;Business Slogan</td>\n",
       "      <td>https://images.signs.com/images/3574EF1B-2E90-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22FABE0F-0A73-484F-8F9E-C9A2942A9DE8</td>\n",
       "      <td>Flood Restoration;remodeling residential;Resid...</td>\n",
       "      <td>Contact Information;Phone Number;Business Name</td>\n",
       "      <td>https://images.signs.com/images/24AD12F4-E49B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5B2C2744-FE53-490C-8EF0-3E5A270DFAF0</td>\n",
       "      <td>Coating;Finishing;Automotive;Manufacturing;war...</td>\n",
       "      <td>Business Name;Contact Information;Phone Number</td>\n",
       "      <td>https://images.signs.com/images/F81C7D6A-EB8A-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2384A84C-D29C-410F-8D44-6583D4E51D91</td>\n",
       "      <td>Consumer;Investment;Portfolio Management;Food;...</td>\n",
       "      <td>Merry Christmas;Holiday;Winter;Holiday</td>\n",
       "      <td>https://images.signs.com/images/BD50BB9C-D28E-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93AC1620-EA83-4B32-9CB2-D879923983E3</td>\n",
       "      <td>Preschool</td>\n",
       "      <td>Now Enrolling;License Number;Business Name;Bus...</td>\n",
       "      <td>https://images.signs.com/images/E1E3EE37-3DF0-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77467</th>\n",
       "      <td>BBAA922E-586E-4EA1-AFC5-67A3A9EFA6B4</td>\n",
       "      <td>Food &amp;amp; Drink;Physical Location;Restaurant</td>\n",
       "      <td>Branded;LOGO;Storefront</td>\n",
       "      <td>https://images.signs.com/images/0256EDF0-5D0C-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77468</th>\n",
       "      <td>7477089F-A7A2-481E-BF83-253CFC22A2C6</td>\n",
       "      <td>Business;professional services;Physical Location</td>\n",
       "      <td>Donation;Check;Information;Branded;LOGO</td>\n",
       "      <td>https://images.signs.com/images/09A1B385-87D7-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77469</th>\n",
       "      <td>19D75A7A-A5A3-4B39-AF54-C2F1DB64B677</td>\n",
       "      <td>Church</td>\n",
       "      <td>christmas;Branded;LOGO;Event;Information;Hours...</td>\n",
       "      <td>https://images.signs.com/images/3F88C5F0-4CE6-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77470</th>\n",
       "      <td>677E9FC2-6C27-4848-A081-7DB6FB40AD54</td>\n",
       "      <td>presbyterian;Church</td>\n",
       "      <td>Branded;LOGO;Storefront</td>\n",
       "      <td>https://images.signs.com/images/694A2136-108C-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77471</th>\n",
       "      <td>12EEBBB2-A7AA-4312-AA05-0902F9ED4733</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>Sign for Home;Labels;Decor</td>\n",
       "      <td>https://images.signs.com/images/260C953D-4E37-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77472 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  User GUID  \\\n",
       "0      5414FF1B-23AE-4AEF-A005-00122A80B8FC   \n",
       "1      22FABE0F-0A73-484F-8F9E-C9A2942A9DE8   \n",
       "2      5B2C2744-FE53-490C-8EF0-3E5A270DFAF0   \n",
       "3      2384A84C-D29C-410F-8D44-6583D4E51D91   \n",
       "4      93AC1620-EA83-4B32-9CB2-D879923983E3   \n",
       "...                                     ...   \n",
       "77467  BBAA922E-586E-4EA1-AFC5-67A3A9EFA6B4   \n",
       "77468  7477089F-A7A2-481E-BF83-253CFC22A2C6   \n",
       "77469  19D75A7A-A5A3-4B39-AF54-C2F1DB64B677   \n",
       "77470  677E9FC2-6C27-4848-A081-7DB6FB40AD54   \n",
       "77471  12EEBBB2-A7AA-4312-AA05-0902F9ED4733   \n",
       "\n",
       "                                               User Tags  \\\n",
       "0                            Marketing;Consulting;Agency   \n",
       "1      Flood Restoration;remodeling residential;Resid...   \n",
       "2      Coating;Finishing;Automotive;Manufacturing;war...   \n",
       "3      Consumer;Investment;Portfolio Management;Food;...   \n",
       "4                                              Preschool   \n",
       "...                                                  ...   \n",
       "77467      Food &amp; Drink;Physical Location;Restaurant   \n",
       "77468   Business;professional services;Physical Location   \n",
       "77469                                             Church   \n",
       "77470                                presbyterian;Church   \n",
       "77471                                           Consumer   \n",
       "\n",
       "                                            Signage Tags  \\\n",
       "0            Business Name;Business Logo;Business Slogan   \n",
       "1         Contact Information;Phone Number;Business Name   \n",
       "2         Business Name;Contact Information;Phone Number   \n",
       "3                 Merry Christmas;Holiday;Winter;Holiday   \n",
       "4      Now Enrolling;License Number;Business Name;Bus...   \n",
       "...                                                  ...   \n",
       "77467                            Branded;LOGO;Storefront   \n",
       "77468            Donation;Check;Information;Branded;LOGO   \n",
       "77469  christmas;Branded;LOGO;Event;Information;Hours...   \n",
       "77470                            Branded;LOGO;Storefront   \n",
       "77471                         Sign for Home;Labels;Decor   \n",
       "\n",
       "                                                     URL  \n",
       "0      https://images.signs.com/images/3574EF1B-2E90-...  \n",
       "1      https://images.signs.com/images/24AD12F4-E49B-...  \n",
       "2      https://images.signs.com/images/F81C7D6A-EB8A-...  \n",
       "3      https://images.signs.com/images/BD50BB9C-D28E-...  \n",
       "4      https://images.signs.com/images/E1E3EE37-3DF0-...  \n",
       "...                                                  ...  \n",
       "77467  https://images.signs.com/images/0256EDF0-5D0C-...  \n",
       "77468  https://images.signs.com/images/09A1B385-87D7-...  \n",
       "77469  https://images.signs.com/images/3F88C5F0-4CE6-...  \n",
       "77470  https://images.signs.com/images/694A2136-108C-...  \n",
       "77471  https://images.signs.com/images/260C953D-4E37-...  \n",
       "\n",
       "[77472 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"image-tags.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new table that uses indices from df in a new column \"path\" that creates values like \"./images/{}.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"image_indices.json\") as f:\n",
    "    image_indices = set(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d290b56b204537aaae40b052d0daa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/77472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from html import unescape\n",
    "\n",
    "new_data = []\n",
    "for i, row in tqdm(\n",
    "    df[df[\"User Tags\"].apply(lambda row: isinstance(row, str))].iterrows(),\n",
    "    total=len(df),\n",
    "):\n",
    "    if i not in image_indices:\n",
    "        continue\n",
    "    user_tags = [tag.strip() for tag in unescape(row[\"User Tags\"]).split(\";\") if tag.strip() != \"\"]\n",
    "    prompt = f'A {\", \".join(user_tags)} sign'\n",
    "    signage_tags_cell = row[\"Signage Tags\"]\n",
    "    if isinstance(signage_tags_cell, str):\n",
    "        signage_tags = [tag.strip() for tag in unescape(signage_tags_cell).split(\";\") if tag.strip() != \"\"]\n",
    "        prompt += f' with {\", \".join(signage_tags)}'\n",
    "    prompt = prompt.lower()\n",
    "    new_data.append(\n",
    "        {\n",
    "            \"image_path\": f\"/home/ubuntu/dev/images/image{i}.png\",\n",
    "            \"caption\": prompt,\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new_data_df\n",
    "new_data_df = pd.DataFrame(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/ubuntu/dev/images/image0.png</td>\n",
       "      <td>a marketing, consulting, agency sign with busi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/ubuntu/dev/images/image1.png</td>\n",
       "      <td>a flood restoration, remodeling residential, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/ubuntu/dev/images/image2.png</td>\n",
       "      <td>a coating, finishing, automotive, manufacturin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/ubuntu/dev/images/image3.png</td>\n",
       "      <td>a consumer, investment, portfolio management, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/ubuntu/dev/images/image4.png</td>\n",
       "      <td>a preschool sign with now enrolling, license n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58128</th>\n",
       "      <td>/home/ubuntu/dev/images/image77467.png</td>\n",
       "      <td>a food &amp; drink, physical location, restaurant ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58129</th>\n",
       "      <td>/home/ubuntu/dev/images/image77468.png</td>\n",
       "      <td>a business, professional services, physical lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58130</th>\n",
       "      <td>/home/ubuntu/dev/images/image77469.png</td>\n",
       "      <td>a church sign with christmas, branded, logo, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58131</th>\n",
       "      <td>/home/ubuntu/dev/images/image77470.png</td>\n",
       "      <td>a presbyterian, church sign with branded, logo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58132</th>\n",
       "      <td>/home/ubuntu/dev/images/image77471.png</td>\n",
       "      <td>a consumer sign with sign for home, labels, decor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58133 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   image_path  \\\n",
       "0          /home/ubuntu/dev/images/image0.png   \n",
       "1          /home/ubuntu/dev/images/image1.png   \n",
       "2          /home/ubuntu/dev/images/image2.png   \n",
       "3          /home/ubuntu/dev/images/image3.png   \n",
       "4          /home/ubuntu/dev/images/image4.png   \n",
       "...                                       ...   \n",
       "58128  /home/ubuntu/dev/images/image77467.png   \n",
       "58129  /home/ubuntu/dev/images/image77468.png   \n",
       "58130  /home/ubuntu/dev/images/image77469.png   \n",
       "58131  /home/ubuntu/dev/images/image77470.png   \n",
       "58132  /home/ubuntu/dev/images/image77471.png   \n",
       "\n",
       "                                                 caption  \n",
       "0      a marketing, consulting, agency sign with busi...  \n",
       "1      a flood restoration, remodeling residential, r...  \n",
       "2      a coating, finishing, automotive, manufacturin...  \n",
       "3      a consumer, investment, portfolio management, ...  \n",
       "4      a preschool sign with now enrolling, license n...  \n",
       "...                                                  ...  \n",
       "58128  a food & drink, physical location, restaurant ...  \n",
       "58129  a business, professional services, physical lo...  \n",
       "58130  a church sign with christmas, branded, logo, e...  \n",
       "58131  a presbyterian, church sign with branded, logo...  \n",
       "58132  a consumer sign with sign for home, labels, decor  \n",
       "\n",
       "[58133 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 15% of the data for testing, and the rest for training\n",
    "test_df = new_data_df.sample(frac=0.15)\n",
    "train_df = new_data_df.drop(test_df.index)\n",
    "# Dump to csv\n",
    "train_df.to_csv(\"train.csv\", index=False)\n",
    "test_df.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-base-patch32 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'visual_projection.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'logit_scale', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_projection.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.bias']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The projection layer and logit scale weights `['visual_projection.weight', 'text_projection.weight', 'logit_scale']` are newly initialized. You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    VisionTextDualEncoderModel, \n",
    "    VisionTextDualEncoderProcessor, \n",
    "    AutoTokenizer, \n",
    "    AutoFeatureExtractor\n",
    ")\n",
    "\n",
    "model = VisionTextDualEncoderModel.from_vision_text_pretrained(\n",
    "    \"openai/clip-vit-base-patch32\", \"roberta-base\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "feat_ext = AutoFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = VisionTextDualEncoderProcessor(feat_ext, tokenizer)\n",
    "\n",
    "# save the model and processor\n",
    "# model.save_pretrained(\"clip-roberta\")\n",
    "# processor.save_pretrained(\"clip-roberta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# model = CLIPModel.from_pretrained(\"./clip-roberta-finetuned/checkpoint-500/\")\n",
    "# processor = CLIPProcessor.from_pretrained(\"./clip-roberta-finetuned/checkpoint-500/\")\n",
    "\n",
    "from transformers import (\n",
    "    VisionTextDualEncoderModel, \n",
    "    VisionTextDualEncoderProcessor, \n",
    "    AutoTokenizer, \n",
    "    AutoFeatureExtractor,\n",
    "    AutoProcessor\n",
    ")\n",
    "\n",
    "model = VisionTextDualEncoderModel.from_pretrained(\n",
    "    \"./clip-roberta-finetuned\"\n",
    ")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"./clip-roberta-finetuned/checkpoint-500/\")\n",
    "# processor = VisionTextDualEncoderProcessor.from_pretrained(\"./clip-roberta\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "feat_ext = AutoFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = VisionTextDualEncoderProcessor(feat_ext, tokenizer)\n",
    "# processor = VisionTextDualEncoderProcessor.from_pretrained(\"./clip-roberta-finetuned/checkpoint-500/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:39: FutureWarning: Pass token='autotagger' as keyword args. From version 0.8 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/huggingface_hub/hf_api.py:641: FutureWarning: `create_repo` now takes `token` as an optional positional argument. Be sure to adapt your code!\n",
      "  warnings.warn(\n",
      "Cloning https://huggingface.co/grantpitt/autotagger into local empty directory.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b628e62790f4abab82cceb2cf96fab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file pytorch_model.bin:   0%|          | 15.6k/812M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d86dc76b433b47c4a55701d9ea285457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file pytorch_model.bin:   0%|          | 1.00k/812M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd6a64f865994ff7b085efef4052fbc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/812M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/grantpitt/autotagger\n",
      "   1ddb475..7e560f0  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/grantpitt/autotagger/commit/7e560f0a65b5542cfa70c044a20955134cbac441'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"autotagger\", use_temp_dir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captions = list(set(test_df[\"caption\"]))\n",
    "# captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"user_tags_set.json\") as f:\n",
    "    # captions = [f'a {item.strip().lower()} sign' for item in json.load(f) if item.strip() != \"\"]\n",
    "    captions = [item.strip().lower() for item in json.load(f) if item.strip() != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['marketing',\n",
       " 'consulting',\n",
       " 'agency',\n",
       " 'flood restoration',\n",
       " 'remodeling residential',\n",
       " 'residential',\n",
       " 'construction',\n",
       " 'coating',\n",
       " 'finishing',\n",
       " 'automotive']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"images/image49547.png\").convert(\"RGB\")\n",
    "\n",
    "inputs = processor(text=captions, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tire service', 'sewing service', 'industrial cleaning',\n",
       "       'facility repair & maintenance', 'branded',\n",
       "       'repair and cleaning services', 'hauling',\n",
       "       'airfield system and service', 'water scooter rental',\n",
       "       'employment branding', 'landscaping services', 'auto detail',\n",
       "       'professional maintenance services', 'mobile repair services',\n",
       "       'diesel engine repair service', 'roofing contractor',\n",
       "       'marketing services', 'power company',\n",
       "       'installation and field service', 'electrical contracting firm'],\n",
       "      dtype='<U50')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(captions)[np.argsort(probs.to(\"cpu\").detach().numpy().flatten())[::-1]][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
